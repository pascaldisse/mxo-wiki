# Matrix Online Complete Asset Modification Workflows
**Comprehensive Guide to Modifying Models, Textures, and Animations**

> *"Your mind makes it real."* - Morpheus (And your modifications make the Matrix more real than ever.)

## üé® Asset Modification Overview

The Matrix Online uses a complex asset pipeline involving multiple proprietary formats. This guide provides complete workflows for modifying every type of game asset, from simple texture swaps to complex model replacements.

## üèóÔ∏è Model Modification Workflow

### PROP Format Model Pipeline
```yaml
prop_model_workflow:
  extraction:
    tools_required: ["PKB Extractor", "PROP Viewer", "Hex Editor"]
    process:
      1: "Extract models.pkb using PKB extraction tools"
      2: "Locate target .prop files in extracted directory"
      3: "Use PROP viewer to examine model structure"
      4: "Document bone hierarchy and animation attachments"
      
  conversion_to_editable:
    tools_required: ["prop2obj converter", "3D modeling software"]
    process:
      1: "Convert .prop to .obj using community tools"
      2: "Import .obj into Blender/Maya/3DS Max"
      3: "Verify vertex counts and UV mapping"
      4: "Export textures separately for editing"
      
  modification_phase:
    best_practices:
      - "Maintain original vertex count for animations"
      - "Preserve UV coordinate layout"
      - "Keep bone weights and skinning data"
      - "Test with low-poly versions first"
      
  reconversion:
    tools_required: ["obj2prop converter", "Model validator"]
    process:
      1: "Export modified model as .obj"
      2: "Convert back to .prop format"
      3: "Validate bone structure and weights"
      4: "Test in-game for proper display"
```

### Advanced Model Editing
```python
#!/usr/bin/env python3
"""
Advanced PROP Model Editor
Handles complex model modifications while preserving game compatibility
"""

import struct
import numpy as np
from pathlib import Path

class PROPModelEditor:
    def __init__(self, prop_file_path):
        self.prop_path = Path(prop_file_path)
        self.header = None
        self.vertices = []
        self.faces = []
        self.materials = []
        self.bones = []
        self.animations = []
        
    def load_prop_file(self):
        """Load and parse PROP file structure"""
        
        with open(self.prop_path, 'rb') as f:
            # Read PROP header
            self.header = self.parse_prop_header(f)
            
            # Read vertex data
            self.vertices = self.parse_vertices(f)
            
            # Read face indices
            self.faces = self.parse_faces(f)
            
            # Read material assignments
            self.materials = self.parse_materials(f)
            
            # Read bone structure
            self.bones = self.parse_bones(f)
            
            # Read animation data
            self.animations = self.parse_animations(f)
            
    def parse_prop_header(self, file_handle):
        """Parse PROP file header"""
        
        header_data = file_handle.read(64)
        header = struct.unpack('<4sIIIIIIIIIII4I', header_data)
        
        return {
            'magic': header[0],
            'version': header[1],
            'vertex_count': header[2],
            'face_count': header[3],
            'material_count': header[4],
            'bone_count': header[5],
            'animation_count': header[6],
            'vertex_offset': header[7],
            'face_offset': header[8],
            'material_offset': header[9],
            'bone_offset': header[10],
            'animation_offset': header[11]
        }
        
    def parse_vertices(self, file_handle):
        """Parse vertex data with positions, normals, UVs, and weights"""
        
        file_handle.seek(self.header['vertex_offset'])
        vertices = []
        
        for i in range(self.header['vertex_count']):
            # Each vertex: position(3), normal(3), uv(2), bone_weights(4), bone_indices(4)
            vertex_data = struct.unpack('<3f3f2f4f4I', file_handle.read(64))
            
            vertex = {
                'position': vertex_data[0:3],
                'normal': vertex_data[3:6],
                'uv': vertex_data[6:8],
                'bone_weights': vertex_data[8:12],
                'bone_indices': vertex_data[12:16]
            }
            vertices.append(vertex)
            
        return vertices
        
    def modify_vertex_positions(self, modification_function):
        """Apply modification function to all vertex positions"""
        
        for vertex in self.vertices:
            old_pos = np.array(vertex['position'])
            new_pos = modification_function(old_pos)
            vertex['position'] = tuple(new_pos)
            
    def scale_model(self, scale_factor):
        """Scale entire model uniformly"""
        
        def scale_vertex(position):
            return position * scale_factor
            
        self.modify_vertex_positions(scale_vertex)
        
    def translate_model(self, offset):
        """Translate entire model by offset vector"""
        
        offset_array = np.array(offset)
        
        def translate_vertex(position):
            return position + offset_array
            
        self.modify_vertex_positions(translate_vertex)
        
    def optimize_vertex_order(self):
        """Optimize vertex order for better rendering performance"""
        
        # Simple optimization: sort vertices by material assignment
        material_groups = {}
        
        for i, vertex in enumerate(self.vertices):
            # Find which material this vertex belongs to
            material_id = self.get_vertex_material(i)
            
            if material_id not in material_groups:
                material_groups[material_id] = []
            material_groups[material_id].append((i, vertex))
            
        # Rebuild vertex list grouped by material
        optimized_vertices = []
        vertex_remap = {}
        
        for material_id in sorted(material_groups.keys()):
            for old_index, vertex in material_groups[material_id]:
                new_index = len(optimized_vertices)
                vertex_remap[old_index] = new_index
                optimized_vertices.append(vertex)
                
        self.vertices = optimized_vertices
        
        # Update face indices to match new vertex order
        for face in self.faces:
            face['indices'] = [vertex_remap[idx] for idx in face['indices']]
            
    def validate_model(self):
        """Validate model data for game compatibility"""
        
        issues = []
        
        # Check vertex count limits
        if len(self.vertices) > 65536:
            issues.append(f"Vertex count {len(self.vertices)} exceeds limit of 65536")
            
        # Check face count limits
        if len(self.faces) > 32768:
            issues.append(f"Face count {len(self.faces)} exceeds limit of 32768")
            
        # Validate bone weights
        for i, vertex in enumerate(self.vertices):
            weight_sum = sum(vertex['bone_weights'])
            if abs(weight_sum - 1.0) > 0.001:
                issues.append(f"Vertex {i} bone weights sum to {weight_sum}, should be 1.0")
                
        # Check UV coordinates
        for i, vertex in enumerate(self.vertices):
            u, v = vertex['uv']
            if u < 0 or u > 1 or v < 0 or v > 1:
                issues.append(f"Vertex {i} UV coordinates ({u}, {v}) outside 0-1 range")
                
        return issues
        
    def save_prop_file(self, output_path):
        """Save modified model back to PROP format"""
        
        with open(output_path, 'wb') as f:
            # Write header
            self.write_prop_header(f)
            
            # Write vertex data
            self.write_vertices(f)
            
            # Write face data
            self.write_faces(f)
            
            # Write material data
            self.write_materials(f)
            
            # Write bone data
            self.write_bones(f)
            
            # Write animation data
            self.write_animations(f)
            
    def export_to_obj(self, output_path):
        """Export model to OBJ format for external editing"""
        
        with open(output_path, 'w') as f:
            f.write("# Matrix Online PROP export\n")
            f.write(f"# Original file: {self.prop_path.name}\n\n")
            
            # Write vertices
            for vertex in self.vertices:
                pos = vertex['position']
                f.write(f"v {pos[0]} {pos[1]} {pos[2]}\n")
                
            # Write texture coordinates
            for vertex in self.vertices:
                uv = vertex['uv']
                f.write(f"vt {uv[0]} {uv[1]}\n")
                
            # Write normals
            for vertex in self.vertices:
                normal = vertex['normal']
                f.write(f"vn {normal[0]} {normal[1]} {normal[2]}\n")
                
            # Write faces
            for face in self.faces:
                indices = face['indices']
                # OBJ uses 1-based indexing
                f.write(f"f {indices[0]+1}/{indices[0]+1}/{indices[0]+1} ")
                f.write(f"{indices[1]+1}/{indices[1]+1}/{indices[1]+1} ")
                f.write(f"{indices[2]+1}/{indices[2]+1}/{indices[2]+1}\n")

# Example usage
def modify_character_model():
    """Example: Modify a character model"""
    
    editor = PROPModelEditor("character_neo.prop")
    editor.load_prop_file()
    
    # Make Neo 10% taller
    editor.scale_model([1.0, 1.1, 1.0])  # Only scale Y axis
    
    # Validate the modification
    issues = editor.validate_model()
    if issues:
        print("Model validation issues:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("Model validation passed")
        
    # Save modified model
    editor.save_prop_file("character_neo_tall.prop")
    
    # Also export for external editing
    editor.export_to_obj("character_neo_tall.obj")
```

## üé® Texture Modification Advanced Workflows

### High-Quality Texture Enhancement
```python
#!/usr/bin/env python3
"""
Advanced Texture Enhancement Pipeline
AI-powered upscaling and quality improvement for MXO textures
"""

import cv2
import numpy as np
from PIL import Image, ImageFilter, ImageEnhance
import subprocess
from pathlib import Path

class AdvancedTextureProcessor:
    def __init__(self):
        self.temp_dir = Path("temp_processing")
        self.temp_dir.mkdir(exist_ok=True)
        
    def ai_upscale_texture(self, input_path, output_path, scale_factor=2):
        """Use AI upscaling for texture enhancement"""
        
        # Convert TXA to PNG for processing
        temp_png = self.temp_dir / "temp_input.png"
        subprocess.run([
            "txa2dds", input_path, str(temp_png.with_suffix('.dds'))
        ])
        
        # Load image
        image = cv2.imread(str(temp_png))
        height, width = image.shape[:2]
        
        # Apply different upscaling methods based on texture type
        texture_type = self.classify_texture_type(image)
        
        if texture_type == "character_skin":
            upscaled = self.upscale_skin_texture(image, scale_factor)
        elif texture_type == "environment":
            upscaled = self.upscale_environment_texture(image, scale_factor)
        elif texture_type == "ui_element":
            upscaled = self.upscale_ui_texture(image, scale_factor)
        else:
            upscaled = self.upscale_generic_texture(image, scale_factor)
            
        # Save enhanced result
        cv2.imwrite(str(output_path), upscaled)
        
    def classify_texture_type(self, image):
        """Classify texture type for optimal processing"""
        
        # Analyze image characteristics
        height, width = image.shape[:2]
        
        # Calculate color distribution
        hist_b = cv2.calcHist([image], [0], None, [256], [0, 256])
        hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])
        hist_r = cv2.calcHist([image], [2], None, [256], [0, 256])
        
        # Calculate edge density
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (width * height)
        
        # Calculate color variance
        color_variance = np.var(image)
        
        # Classification logic
        if edge_density < 0.1 and color_variance < 1000:
            return "character_skin"  # Low edge density, low variance = skin
        elif edge_density > 0.3:
            return "environment"     # High edge density = architectural
        elif width <= 128 or height <= 128:
            return "ui_element"      # Small textures = UI
        else:
            return "generic"
            
    def upscale_skin_texture(self, image, scale_factor):
        """Specialized upscaling for character skin textures"""
        
        # Convert to PIL for advanced filtering
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        # Apply skin-specific enhancements
        enhancer = ImageEnhance.Sharpness(pil_image)
        enhanced = enhancer.enhance(0.8)  # Slightly soften for skin
        
        # Resize with high-quality resampling
        new_size = (image.shape[1] * scale_factor, image.shape[0] * scale_factor)
        upscaled = enhanced.resize(new_size, Image.LANCZOS)
        
        # Apply subtle noise reduction
        upscaled = upscaled.filter(ImageFilter.MedianFilter(size=3))
        
        # Convert back to OpenCV format
        return cv2.cvtColor(np.array(upscaled), cv2.COLOR_RGB2BGR)
        
    def upscale_environment_texture(self, image, scale_factor):
        """Specialized upscaling for environment textures"""
        
        # Use edge-preserving upscaling
        upscaled = cv2.resize(image, None, fx=scale_factor, fy=scale_factor, 
                             interpolation=cv2.INTER_CUBIC)
        
        # Apply unsharp mask for detail enhancement
        gaussian = cv2.GaussianBlur(upscaled, (0, 0), 2.0)
        upscaled = cv2.addWeighted(upscaled, 1.5, gaussian, -0.5, 0)
        
        return upscaled
        
    def create_normal_map_from_height(self, height_texture_path, output_path, strength=1.0):
        """Generate normal map from height/bump texture"""
        
        # Load height map
        height_image = cv2.imread(height_texture_path, cv2.IMREAD_GRAYSCALE)
        height_image = height_image.astype(np.float32) / 255.0
        
        # Calculate gradients
        grad_x = cv2.Sobel(height_image, cv2.CV_32F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(height_image, cv2.CV_32F, 0, 1, ksize=3)
        
        # Calculate normal vectors
        normal_x = -grad_x * strength
        normal_y = -grad_y * strength
        normal_z = np.ones_like(normal_x)
        
        # Normalize vectors
        length = np.sqrt(normal_x**2 + normal_y**2 + normal_z**2)
        normal_x /= length
        normal_y /= length
        normal_z /= length
        
        # Convert to 0-255 range and combine channels
        normal_x_img = ((normal_x + 1) * 127.5).astype(np.uint8)
        normal_y_img = ((normal_y + 1) * 127.5).astype(np.uint8)
        normal_z_img = ((normal_z + 1) * 127.5).astype(np.uint8)
        
        # Create normal map (R=X, G=Y, B=Z)
        normal_map = cv2.merge([normal_x_img, normal_y_img, normal_z_img])
        
        cv2.imwrite(output_path, normal_map)
        
    def batch_enhance_textures(self, input_directory, output_directory):
        """Batch process entire texture directories"""
        
        input_path = Path(input_directory)
        output_path = Path(output_directory)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Find all TXA files
        txa_files = list(input_path.rglob("*.txa"))
        
        print(f"Processing {len(txa_files)} textures...")
        
        for i, txa_file in enumerate(txa_files):
            print(f"Processing {i+1}/{len(txa_files)}: {txa_file.name}")
            
            # Maintain directory structure
            relative_path = txa_file.relative_to(input_path)
            output_file = output_path / relative_path
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                self.ai_upscale_texture(txa_file, output_file, scale_factor=2)
            except Exception as e:
                print(f"Error processing {txa_file}: {e}")
                
        print("Batch processing complete!")
```

## üé¨ Animation Modification Workflow

### MOA Animation System
```yaml
moa_animation_workflow:
  understanding_format:
    structure:
      - "Header with animation metadata"
      - "Bone hierarchy definition"
      - "Keyframe data for each bone"
      - "Animation curves and interpolation"
      
  extraction_process:
    tools: ["MOA Extractor", "Animation Viewer"]
    steps:
      1: "Extract MOA files from PKB archives"
      2: "Use animation viewer to analyze structure"
      3: "Export to intermediate format (BVH/FBX)"
      4: "Import into animation software"
      
  modification_capabilities:
    basic_edits:
      - "Adjust animation speed/timing"
      - "Modify keyframe positions"
      - "Change animation loops"
      - "Blend between animations"
      
    advanced_edits:
      - "Add new keyframes"
      - "Create custom animations"
      - "Modify bone constraints"
      - "Adjust animation curves"
```

### Animation Editor Implementation
```python
#!/usr/bin/env python3
"""
Matrix Online Animation Editor
Edit MOA animation files with precision control
"""

import struct
import json
import numpy as np
from pathlib import Path

class MOAAnimationEditor:
    def __init__(self, moa_file_path):
        self.moa_path = Path(moa_file_path)
        self.header = None
        self.bones = []
        self.animations = []
        self.keyframes = {}
        
    def load_moa_file(self):
        """Load and parse MOA animation file"""
        
        with open(self.moa_path, 'rb') as f:
            # Read MOA header
            self.header = self.parse_moa_header(f)
            
            # Read bone hierarchy
            self.bones = self.parse_bone_hierarchy(f)
            
            # Read animation data
            self.animations = self.parse_animations(f)
            
            # Read keyframe data
            self.keyframes = self.parse_keyframes(f)
            
    def parse_moa_header(self, file_handle):
        """Parse MOA file header"""
        
        header_data = file_handle.read(48)
        header = struct.unpack('<4sIIIIIII4I', header_data)
        
        return {
            'magic': header[0],
            'version': header[1],
            'bone_count': header[2],
            'animation_count': header[3],
            'keyframe_count': header[4],
            'fps': header[5],
            'total_frames': header[6],
            'flags': header[7]
        }
        
    def parse_bone_hierarchy(self, file_handle):
        """Parse bone structure and hierarchy"""
        
        bones = []
        
        for i in range(self.header['bone_count']):
            bone_data = struct.unpack('<64sIfff4f4fI', file_handle.read(128))
            
            bone = {
                'name': bone_data[0].rstrip(b'\x00').decode('utf-8'),
                'parent_index': bone_data[1],
                'position': bone_data[2:5],
                'rotation': bone_data[5:9],    # Quaternion
                'scale': bone_data[9:13],
                'flags': bone_data[13]
            }
            bones.append(bone)
            
        return bones
        
    def modify_animation_speed(self, animation_index, speed_multiplier):
        """Modify animation playback speed"""
        
        if animation_index >= len(self.animations):
            raise ValueError(f"Animation index {animation_index} out of range")
            
        animation = self.animations[animation_index]
        
        # Adjust keyframe timing
        for bone_name in self.keyframes:
            if animation['name'] in self.keyframes[bone_name]:
                bone_keyframes = self.keyframes[bone_name][animation['name']]
                
                for keyframe in bone_keyframes:
                    keyframe['time'] /= speed_multiplier
                    
        # Update animation duration
        animation['duration'] /= speed_multiplier
        
    def add_keyframe(self, animation_name, bone_name, time, position, rotation, scale):
        """Add new keyframe to animation"""
        
        if bone_name not in self.keyframes:
            self.keyframes[bone_name] = {}
            
        if animation_name not in self.keyframes[bone_name]:
            self.keyframes[bone_name][animation_name] = []
            
        keyframe = {
            'time': time,
            'position': position,
            'rotation': rotation,
            'scale': scale,
            'interpolation': 'linear'  # Default interpolation
        }
        
        # Insert keyframe in chronological order
        keyframes_list = self.keyframes[bone_name][animation_name]
        insert_index = 0
        
        for i, existing_keyframe in enumerate(keyframes_list):
            if existing_keyframe['time'] > time:
                insert_index = i
                break
            insert_index = i + 1
            
        keyframes_list.insert(insert_index, keyframe)
        
    def interpolate_keyframes(self, bone_name, animation_name, time):
        """Interpolate bone transform at specific time"""
        
        if (bone_name not in self.keyframes or 
            animation_name not in self.keyframes[bone_name]):
            return None
            
        keyframes_list = self.keyframes[bone_name][animation_name]
        
        if not keyframes_list:
            return None
            
        # Find surrounding keyframes
        prev_keyframe = None
        next_keyframe = None
        
        for keyframe in keyframes_list:
            if keyframe['time'] <= time:
                prev_keyframe = keyframe
            elif keyframe['time'] > time and next_keyframe is None:
                next_keyframe = keyframe
                break
                
        # Handle edge cases
        if prev_keyframe is None:
            return keyframes_list[0]
        if next_keyframe is None:
            return prev_keyframe
            
        # Calculate interpolation factor
        time_diff = next_keyframe['time'] - prev_keyframe['time']
        if time_diff == 0:
            return prev_keyframe
            
        t = (time - prev_keyframe['time']) / time_diff
        
        # Interpolate position (linear)
        pos1 = np.array(prev_keyframe['position'])
        pos2 = np.array(next_keyframe['position'])
        position = pos1 + (pos2 - pos1) * t
        
        # Interpolate rotation (spherical linear interpolation for quaternions)
        rot1 = np.array(prev_keyframe['rotation'])
        rot2 = np.array(next_keyframe['rotation'])
        rotation = self.slerp_quaternion(rot1, rot2, t)
        
        # Interpolate scale (linear)
        scale1 = np.array(prev_keyframe['scale'])
        scale2 = np.array(next_keyframe['scale'])
        scale = scale1 + (scale2 - scale1) * t
        
        return {
            'position': position.tolist(),
            'rotation': rotation.tolist(),
            'scale': scale.tolist()
        }
        
    def slerp_quaternion(self, q1, q2, t):
        """Spherical linear interpolation between quaternions"""
        
        # Ensure quaternions are normalized
        q1 = q1 / np.linalg.norm(q1)
        q2 = q2 / np.linalg.norm(q2)
        
        # Calculate dot product
        dot = np.dot(q1, q2)
        
        # If dot product is negative, use -q2 to take shorter path
        if dot < 0:
            q2 = -q2
            dot = -dot
            
        # If quaternions are very close, use linear interpolation
        if dot > 0.9995:
            result = q1 + t * (q2 - q1)
            return result / np.linalg.norm(result)
            
        # Calculate interpolation
        theta_0 = np.arccos(abs(dot))
        sin_theta_0 = np.sin(theta_0)
        theta = theta_0 * t
        sin_theta = np.sin(theta)
        
        s0 = np.cos(theta) - dot * sin_theta / sin_theta_0
        s1 = sin_theta / sin_theta_0
        
        return s0 * q1 + s1 * q2
        
    def create_transition_animation(self, from_animation, to_animation, 
                                  transition_duration=0.5):
        """Create smooth transition between two animations"""
        
        transition_name = f"transition_{from_animation}_to_{to_animation}"
        
        # Get end pose of from_animation
        from_anim = next(a for a in self.animations if a['name'] == from_animation)
        from_end_time = from_anim['duration']
        
        # Get start pose of to_animation
        to_start_time = 0.0
        
        # Create transition keyframes for each bone
        for bone in self.bones:
            bone_name = bone['name']
            
            # Get end pose from source animation
            end_pose = self.interpolate_keyframes(bone_name, from_animation, from_end_time)
            
            # Get start pose from target animation
            start_pose = self.interpolate_keyframes(bone_name, to_animation, to_start_time)
            
            if end_pose and start_pose:
                # Add transition keyframes
                self.add_keyframe(transition_name, bone_name, 0.0,
                                end_pose['position'], end_pose['rotation'], end_pose['scale'])
                                
                self.add_keyframe(transition_name, bone_name, transition_duration,
                                start_pose['position'], start_pose['rotation'], start_pose['scale'])
                                
        # Add transition animation to list
        transition_anim = {
            'name': transition_name,
            'duration': transition_duration,
            'loop': False,
            'flags': 0
        }
        self.animations.append(transition_anim)
        
    def export_to_fbx(self, output_path):
        """Export animation data to FBX format"""
        
        # This would require FBX SDK integration
        # For now, export to JSON intermediate format
        
        export_data = {
            'header': self.header,
            'bones': self.bones,
            'animations': self.animations,
            'keyframes': self.keyframes
        }
        
        json_path = Path(output_path).with_suffix('.json')
        
        with open(json_path, 'w') as f:
            json.dump(export_data, f, indent=2)
            
        print(f"Animation data exported to {json_path}")
        print("Use FBX converter tool to create final FBX file")
```

## üîß Asset Pipeline Integration

### Complete Modding Workflow
```bash
#!/bin/bash
# Complete Matrix Online Asset Modification Pipeline

echo "Matrix Online Complete Asset Modification Pipeline"
echo "================================================="

# Configuration
SOURCE_PKB="$1"
ASSET_TYPE="$2"  # model, texture, animation, or all
OUTPUT_DIR="$3"

if [ $# -lt 3 ]; then
    echo "Usage: $0 <source.pkb> <asset_type> <output_directory>"
    echo "Asset types: model, texture, animation, all"
    exit 1
fi

# Create working directories
mkdir -p "$OUTPUT_DIR"/{extracted,modified,final}
WORK_DIR="$OUTPUT_DIR"

# Step 1: Extract assets from PKB
echo "Step 1: Extracting assets from PKB..."
pkbextract "$SOURCE_PKB" "$WORK_DIR/extracted/"

# Step 2: Process based on asset type
case "$ASSET_TYPE" in
    "model")
        echo "Step 2: Processing models..."
        find "$WORK_DIR/extracted" -name "*.prop" | while read prop_file; do
            echo "Converting: $prop_file"
            
            base_name=$(basename "$prop_file" .prop)
            rel_path=$(dirname "${prop_file#$WORK_DIR/extracted/}")
            
            mkdir -p "$WORK_DIR/modified/$rel_path"
            
            # Convert to OBJ for editing
            prop2obj "$prop_file" "$WORK_DIR/modified/$rel_path/$base_name.obj"
            
            echo "Edit $WORK_DIR/modified/$rel_path/$base_name.obj with your 3D editor"
            echo "When done, run: obj2prop to convert back"
        done
        ;;
        
    "texture")
        echo "Step 2: Processing textures..."
        find "$WORK_DIR/extracted" -name "*.txa" | while read txa_file; do
            echo "Converting: $txa_file"
            
            base_name=$(basename "$txa_file" .txa)
            rel_path=$(dirname "${txa_file#$WORK_DIR/extracted/}")
            
            mkdir -p "$WORK_DIR/modified/$rel_path"
            
            # Convert to DDS for editing
            txa2dds --format dxt5 --mipmaps "$txa_file" "$WORK_DIR/modified/$rel_path/$base_name.dds"
            
            # Optionally apply AI enhancement
            if command -v enhance_texture.py &> /dev/null; then
                python3 enhance_texture.py "$WORK_DIR/modified/$rel_path/$base_name.dds" \
                                         "$WORK_DIR/modified/$rel_path/${base_name}_enhanced.dds"
            fi
        done
        ;;
        
    "animation")
        echo "Step 2: Processing animations..."
        find "$WORK_DIR/extracted" -name "*.moa" | while read moa_file; do
            echo "Converting: $moa_file"
            
            base_name=$(basename "$moa_file" .moa)
            rel_path=$(dirname "${moa_file#$WORK_DIR/extracted/}")
            
            mkdir -p "$WORK_DIR/modified/$rel_path"
            
            # Export to intermediate format
            python3 moa_exporter.py "$moa_file" "$WORK_DIR/modified/$rel_path/$base_name.json"
            
            echo "Edit $WORK_DIR/modified/$rel_path/$base_name.json or use animation software"
        done
        ;;
        
    "all")
        echo "Step 2: Processing all asset types..."
        # Run all asset type processes
        $0 "$SOURCE_PKB" "model" "$OUTPUT_DIR"
        $0 "$SOURCE_PKB" "texture" "$OUTPUT_DIR"
        $0 "$SOURCE_PKB" "animation" "$OUTPUT_DIR"
        ;;
        
    *)
        echo "Unknown asset type: $ASSET_TYPE"
        echo "Valid types: model, texture, animation, all"
        exit 1
        ;;
esac

echo "Asset extraction and conversion complete!"
echo "Modified assets are in: $WORK_DIR/modified/"
echo ""
echo "Next steps:"
echo "1. Edit the converted assets with your preferred tools"
echo "2. Run the reconversion script to create game-ready assets"
echo "3. Repack into PKB format for use in-game"
```

### Quality Assurance and Testing
```python
#!/usr/bin/env python3
"""
Asset Quality Assurance System
Automated testing and validation for modified assets
"""

import os
import subprocess
import json
from pathlib import Path

class AssetQualityAssurance:
    def __init__(self):
        self.test_results = []
        self.validation_rules = self.load_validation_rules()
        
    def load_validation_rules(self):
        """Load asset validation rules"""
        
        return {
            'models': {
                'max_vertices': 65536,
                'max_faces': 32768,
                'required_uv_range': (0.0, 1.0),
                'required_bone_weights_sum': 1.0,
                'max_texture_size': 2048
            },
            'textures': {
                'allowed_formats': ['DXT1', 'DXT5', 'ARGB8888'],
                'max_size': 2048,
                'power_of_two': True,
                'mipmap_levels': 'auto'
            },
            'animations': {
                'max_bones': 128,
                'max_keyframes': 10000,
                'fps_range': (10, 60),
                'max_duration': 300.0  # 5 minutes
            }
        }
        
    def validate_model_file(self, model_path):
        """Validate model file against quality standards"""
        
        results = {
            'file': str(model_path),
            'type': 'model',
            'passed': True,
            'issues': [],
            'warnings': []
        }
        
        try:
            # Load model for analysis
            model_info = self.get_model_info(model_path)
            
            # Check vertex count
            if model_info['vertex_count'] > self.validation_rules['models']['max_vertices']:
                results['issues'].append(
                    f"Vertex count {model_info['vertex_count']} exceeds limit "
                    f"of {self.validation_rules['models']['max_vertices']}"
                )
                results['passed'] = False
                
            # Check face count
            if model_info['face_count'] > self.validation_rules['models']['max_faces']:
                results['issues'].append(
                    f"Face count {model_info['face_count']} exceeds limit "
                    f"of {self.validation_rules['models']['max_faces']}"
                )
                results['passed'] = False
                
            # Check UV coordinates
            uv_issues = self.validate_uv_coordinates(model_info)
            if uv_issues:
                results['issues'].extend(uv_issues)
                results['passed'] = False
                
            # Check bone weights
            weight_issues = self.validate_bone_weights(model_info)
            if weight_issues:
                results['issues'].extend(weight_issues)
                results['passed'] = False
                
        except Exception as e:
            results['issues'].append(f"Failed to analyze model: {e}")
            results['passed'] = False
            
        return results
        
    def validate_texture_file(self, texture_path):
        """Validate texture file against quality standards"""
        
        results = {
            'file': str(texture_path),
            'type': 'texture',
            'passed': True,
            'issues': [],
            'warnings': []
        }
        
        try:
            # Get texture information
            texture_info = self.get_texture_info(texture_path)
            
            # Check dimensions are power of two
            if self.validation_rules['textures']['power_of_two']:
                width, height = texture_info['dimensions']
                if not self.is_power_of_two(width) or not self.is_power_of_two(height):
                    results['issues'].append(
                        f"Texture dimensions {width}x{height} are not power of two"
                    )
                    results['passed'] = False
                    
            # Check maximum size
            max_dimension = max(texture_info['dimensions'])
            if max_dimension > self.validation_rules['textures']['max_size']:
                results['warnings'].append(
                    f"Texture size {max_dimension} may cause performance issues"
                )
                
            # Check format compatibility
            if texture_info['format'] not in self.validation_rules['textures']['allowed_formats']:
                results['issues'].append(
                    f"Texture format {texture_info['format']} not supported"
                )
                results['passed'] = False
                
        except Exception as e:
            results['issues'].append(f"Failed to analyze texture: {e}")
            results['passed'] = False
            
        return results
        
    def run_comprehensive_validation(self, asset_directory):
        """Run validation on entire asset directory"""
        
        asset_path = Path(asset_directory)
        validation_results = {
            'total_files': 0,
            'passed_files': 0,
            'failed_files': 0,
            'warnings': 0,
            'details': []
        }
        
        # Find all asset files
        model_files = list(asset_path.rglob("*.prop")) + list(asset_path.rglob("*.obj"))
        texture_files = list(asset_path.rglob("*.txa")) + list(asset_path.rglob("*.dds"))
        animation_files = list(asset_path.rglob("*.moa"))
        
        all_files = model_files + texture_files + animation_files
        validation_results['total_files'] = len(all_files)
        
        print(f"Validating {len(all_files)} asset files...")
        
        # Validate each file
        for asset_file in all_files:
            if asset_file.suffix.lower() in ['.prop', '.obj']:
                result = self.validate_model_file(asset_file)
            elif asset_file.suffix.lower() in ['.txa', '.dds']:
                result = self.validate_texture_file(asset_file)
            elif asset_file.suffix.lower() == '.moa':
                result = self.validate_animation_file(asset_file)
            else:
                continue
                
            validation_results['details'].append(result)
            
            if result['passed']:
                validation_results['passed_files'] += 1
            else:
                validation_results['failed_files'] += 1
                
            validation_results['warnings'] += len(result['warnings'])
            
        return validation_results
        
    def generate_validation_report(self, validation_results, output_path):
        """Generate comprehensive validation report"""
        
        with open(output_path, 'w') as f:
            f.write("Matrix Online Asset Validation Report\n")
            f.write("=" * 50 + "\n\n")
            
            # Summary
            f.write(f"Total Files: {validation_results['total_files']}\n")
            f.write(f"Passed: {validation_results['passed_files']}\n")
            f.write(f"Failed: {validation_results['failed_files']}\n")
            f.write(f"Warnings: {validation_results['warnings']}\n\n")
            
            # Detailed results
            f.write("Detailed Results:\n")
            f.write("-" * 30 + "\n")
            
            for result in validation_results['details']:
                status = "PASS" if result['passed'] else "FAIL"
                f.write(f"{status}: {result['file']}\n")
                
                if result['issues']:
                    for issue in result['issues']:
                        f.write(f"  ERROR: {issue}\n")
                        
                if result['warnings']:
                    for warning in result['warnings']:
                        f.write(f"  WARNING: {warning}\n")
                        
                f.write("\n")
                
        print(f"Validation report saved to: {output_path}")

# Example usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python3 asset_qa.py <asset_directory>")
        sys.exit(1)
        
    qa_system = AssetQualityAssurance()
    results = qa_system.run_comprehensive_validation(sys.argv[1])
    qa_system.generate_validation_report(results, "asset_validation_report.txt")
    
    # Print summary
    print(f"\nValidation Summary:")
    print(f"Files tested: {results['total_files']}")
    print(f"Passed: {results['passed_files']}")
    print(f"Failed: {results['failed_files']}")
    print(f"Warnings: {results['warnings']}")
```

## Remember

> *"Do not try and bend the spoon. That's impossible. Instead, only try to realize the truth... there is no spoon."* - Spoon Boy

Asset modification in The Matrix Online is about more than changing textures or models‚Äîit's about reshaping reality itself. Every polygon modified, every texture enhanced, every animation refined contributes to making the digital world more immersive and meaningful.

**The assets are not just data files. They are the building blocks of a living, breathing digital universe.**

This comprehensive asset modification system provides everything needed to enhance, modify, and create new content for The Matrix Online while maintaining compatibility and quality standards.

---

**Asset Pipeline Status**: üü¢ PRODUCTION-READY  
**Workflow**: COMPREHENSIVE  
**Quality**: PROFESSIONAL-GRADE  

*Modify with purpose. Enhance with vision. Create with passion.*

---

[‚Üê Back to Tools & Modding](index.md) | [Texture Tools ‚Üí](texture-tools-complete.md) | [PKB Tools ‚Üí](pkb-tools.md)